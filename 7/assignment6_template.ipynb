{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db65d851",
   "metadata": {},
   "source": [
    "# Assignment 6: RAG with Ollama\n",
    "\n",
    "In this assignment, we explore **Retrieval-augmented generation (RAG)** with Large Language Models (LLMs). **This assignment is completely manually graded.**\n",
    "\n",
    "Before you start, please be adivsed that you will need [Ollama](https://ollama.com/) and quite a few Python packages. It is therefore recommended that you use the [JupyterHub at Minerva cluster](https://git.chalmers.se/karppa/minerva/) as **Ollama is already running on the server and all required packges are installed**. See instructions behind the link to see how to gain access.\n",
    "\n",
    "The next section describes how to install the prerequisites on your local computer. You can skip it if you are using Minerva.\n",
    "\n",
    "## Prerequisites (only for local installation)\n",
    "\n",
    "**You can skip this section if you use Minerva.**\n",
    "\n",
    "To run this assignment locally, you are going to need to install quite a few prerequisites. Most of these can be installed on Conda throught the conda-forge channel (`conda install -c conda-forge` *package name*). It is recommended that you create a new environment for this purpose. Install the following packages through conda forge:\n",
    "- `langchain`\n",
    "- `langchain-community`\n",
    "- `langchain-huggingface`\n",
    "- `langchain-chroma`\n",
    "- `langchain-ollama`\n",
    "- `pypdf`\n",
    "- `chromadb`\n",
    "\n",
    "The package `langchain-ollama` is also needed but is not available through Conda. Install it with `pip`:\n",
    "\n",
    "```python3 -m pip install langchain-ollama```\n",
    "\n",
    "Finally, you will need to install [Ollama](https://ollama.com/). It is also available through Conda, but the version served there is rather dated and does not support newer LLMs (it works fine with [Mistral](https://mistral.ai/) 7B, though).\n",
    "\n",
    "Ollama is open source server software that runs LLMs locally on your computer. You can choose which LLM you want to use. By default, the code below uses [Llama](https://www.llama.com/) 3.2 3B, but if this is slow, you can switch to Llama 3.2 1B, or if you want to use an European model (or want to use an older version of Ollama), you can try Mistral 7B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708d098",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Below, you can see a class that has been named `RAGAssistant`. It provides a convenient access to try out RAG. It works as follows:\n",
    "- The constructor simply initializes the class (nothing fancy).\n",
    "- `add_document(filename)` adds the given document file into the database. The document must either be raw UTF-8 text (`.txt`) or a PDF file (`.pdf`). What it does is that it extracts the text from the document, converts it into chunks, embeds them into an LLM vector space, and then stores them in a [Chroma](https://www.trychroma.com/) database.\n",
    "- `query(query_text)` queries the database using the specified LLM. What it does is that it first queries the closest documents matching the query from the database, injects these as a context into the prompt presented to the LLM, and then presents the query to the LLM.\n",
    "- `reset()` resets the database.\n",
    "\n",
    "The class can be used in an interactive fashion, so once you've added documents, you can start making as many queries you want. When you want to remove the documents from the database, just use `reset`, and add some new documents instead.\n",
    "\n",
    "The choice of the LLM is controlled by the variable `LLM`.\n",
    "**If you are running Ollama locally**, you must `pull` the LLM by issuing the following command on the command line:\n",
    "\n",
    "```ollama pull llm-name```\n",
    "\n",
    "E.g., `ollama pull llama3.2:3b` pulls (dowloads from the Internet) the default LLM we might want to use.\n",
    "\n",
    "The following models have been installed on Minerva:\n",
    "- `mistral:7b`\n",
    "- `llama3.1:8b`\n",
    "- `llama3.2:1b`\n",
    "- `llama3.2:3b`\n",
    "\n",
    "The system is very simple and does not, for example, contain any memory to process multiple queries. However, it is very easy to extend this to more complex applications.\n",
    "\n",
    "**Beware:** LLMs are very computationally demanding, and the queries may be slow. Furthermore, in some cases, the queries can take an extraordinary amount of time in which case it might be wise to interrupt the query and rephrase. `llama3.2:1b` is probably the fastest of the bunch, which of course means that it performs the poorest, but any one will do for our purposes.\n",
    "\n",
    "**Beware:** Adding documents into the database can take a lot of time because of the expensive embedding computations. When testing the Question 3 on Minerva, it took **over 7 minutes** to add the entire dataset (all PDF files). **This is normal.** However, you only need to add the documents once during a session and then you can query as many prompts as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGAssistant:\n",
    "    \"\"\"\n",
    "    Performs queries about documents using Mistral 7B model with Ollama\n",
    "    \"\"\"\n",
    "\n",
    "    CHROMA_PATH = 'chroma'\n",
    "    EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Answer the question based on the above context: {question}\n",
    "    \"\"\"\n",
    "    # LLM = 'mistral:7b'\n",
    "    # LLM = 'llama3.1:8b'\n",
    "    # LLM = 'llama3.2:1b'\n",
    "    LLM = 'llama3.2:3b'\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialization: simply reset the database (remove it)\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=80,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Empties the database\n",
    "        \"\"\"\n",
    "        self.db = Chroma(embedding_function = self.get_embedding_function())\n",
    "        self.db.reset_collection()\n",
    "\n",
    "    \n",
    "    def add_document(self, filename):\n",
    "        \"\"\"\n",
    "        Adds the given document into the database. The file must be either a PDF or a .txt file.\n",
    "        If filename has no extension, it is assumed to be a directory and all PDF files are loaded\n",
    "        from the directory.\n",
    "        \"\"\"\n",
    "        if filename.endswith('.pdf'):\n",
    "            documents = PyPDFLoader(filename).load()\n",
    "        elif filename.endswith('.txt'):\n",
    "            documents = TextLoader(filename).load()\n",
    "        elif os.path.isdir(filename):\n",
    "            documents = PyPDFDirectoryLoader(filename).load()\n",
    "        else:\n",
    "            raise ValueError('Filename must end in either .pdf or .txt, '\n",
    "                             'or be a directory, '\n",
    "                                 f'but got `{filename}\\'')\n",
    "            \n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        chunks_with_ids = self.calculate_chunk_ids(chunks)\n",
    "\n",
    "        existing_items = self.db.get(include=[])\n",
    "        existing_ids = set(existing_items[\"ids\"])\n",
    "\n",
    "        # Only add documents that don't exist in the DB.\n",
    "        new_chunks = []\n",
    "        for chunk in chunks_with_ids:\n",
    "            if chunk.metadata[\"id\"] not in existing_ids:\n",
    "                new_chunks.append(chunk)\n",
    "\n",
    "        if len(new_chunks):\n",
    "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "            # self.db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            self._add_in_batches(new_chunks, new_chunk_ids, batch_size=5000)\n",
    "\n",
    "    def _add_in_batches(self, docs: List[Document], ids: List[str], batch_size: int = 5000):\n",
    "        \"\"\"\n",
    "        Add documents to Chroma in fixed-size batches to avoid backend limits.\n",
    "        \"\"\"\n",
    "        assert len(docs) == len(ids)\n",
    "        for i in range(0, len(docs), batch_size):\n",
    "            batch_docs = docs[i : i + batch_size]\n",
    "            batch_ids = ids[i : i + batch_size]\n",
    "            self.db.add_documents(batch_docs, ids=batch_ids)\n",
    "\n",
    "\n",
    "    def calculate_chunk_ids(self, chunks):\n",
    "        \"\"\"\n",
    "        Helper function for computing chunk ids\n",
    "        \"\"\"\n",
    "        last_page_id = None\n",
    "        current_chunk_index = 0\n",
    "\n",
    "        for chunk in chunks:\n",
    "            source = chunk.metadata.get(\"source\")\n",
    "            page = chunk.metadata.get(\"page\")\n",
    "            current_page_id = f\"{source}:{page}\"\n",
    "            if current_page_id == last_page_id:\n",
    "                current_chunk_index += 1\n",
    "            else:\n",
    "                current_chunk_index = 0\n",
    "            chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "            last_page_id = current_page_id\n",
    "            chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def get_embedding_function(self):\n",
    "        \"\"\"\n",
    "        Helper function for simply returning the same embedding function always\n",
    "        \"\"\"\n",
    "        return HuggingFaceEmbeddings(model_name=self.EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    def query(self, query_text):\n",
    "        \"\"\"\n",
    "        Performs the RAG query.\n",
    "        \n",
    "        First, queries the database for the most relevant documents.\n",
    "        \n",
    "        Then, injects the documents into the prompt.\n",
    "        \n",
    "        Finally, invokes the LLM to answer the prompt within the context.\n",
    "        \n",
    "        Return value: (response,sources) where response is a string and sources is a list of sources\n",
    "        \"\"\"\n",
    "        \n",
    "        results = self.db.similarity_search_with_score(query_text, k=5)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        prompt_template = ChatPromptTemplate.from_template(self.PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text,\n",
    "                                            question=query_text)\n",
    "        \n",
    "        model = ChatOllama(model = self.LLM)\n",
    "        response_text = model.invoke(prompt)\n",
    "\n",
    "        sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "        return (response_text.content,sources)\n",
    "\n",
    "\n",
    "# this is the object we're going to manipulate\n",
    "rag = RAGAssistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed274a9",
   "metadata": {},
   "source": [
    "## Question 1: EU AI Act\n",
    "\n",
    "Thhe file `ai_act_en.pdf` (available on Canvas) contains the text of the EU AI Act in English. Use your `rag` object to add the file into the database. Then, choose a question you think is interesting regarding the AI act and explore what the model says about it.\n",
    "\n",
    "Potential themes can be the aims of the AI act, the impact that the act has on businesses, the impact on particular industries (such as healthcare or finance), or potential issues with the act, or something completely different but relevant. Don't know about the AI act? Now it's time to find out.\n",
    "\n",
    "Check what documents the model has used for sources. Do the cited sources support the claims the model makes? Once you've probed your model about the act, cross-reference the models answers with other sources you can find online. Do they confirm what the model is saying or are they contradictory, or are the model's answers even nonsensical?\n",
    "\n",
    "Write 2–4 paragraphs. Include in your answer at least one prompt and answer that your model gave you. Cite the sources you used to confirm or contradict what the model claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f81e36",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use as many code cells here as you want, this is manually graded\n",
    "rag.add_document(\"ai_act_en.pdf\")\n",
    "response = rag.query(\"What is EU for AI action?\")\n",
    "print(response[0])\n",
    "print(response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a67870",
   "metadata": {},
   "source": [
    "Prompt given to the model: What is EU for AI action?\n",
    "\n",
    "Answer the model returned:According to the provided context, the European Union's (EU) \"for AI action\" seems to refer to the implementation of measures to promote trustworthy and ethical Artificial Intelligence (AI). This includes:\n",
    "\n",
    "1. Establishing a European Artificial Intelligence Board to support the Commission in promoting AI literacy tools, public awareness, and understanding of AI benefits, risks, safeguards, rights, and obligations.\n",
    "2. Facilitating the development of voluntary codes of conduct to advance AI literacy among persons dealing with AI development, operation, and use.\n",
    "3. Supporting innovation with a focus on small and medium-sized enterprises (SMEs), including startups, by laying down rules and measures that promote a human-centered approach to AI.\n",
    "4. Ensuring the protection of ethical principles, as specifically requested by the European Parliament.\n",
    "\n",
    "In summary, the EU's \"for AI action\" seems to aim at promoting trustworthiness, ethics, and innovation in AI through regulatory measures, public awareness, and collaboration with stakeholders.\n",
    "\n",
    "the model reported using these internal refs: ['ai_act_en.pdf:5:3', 'ai_act_en.pdf:1:6', 'ai_act_en.pdf:17:6', 'ai_act_en.pdf:37:0', 'ai_act_en.pdf:34:5']\n",
    "\n",
    "The model’s summary accurately reflects the main objectives of the EU AI Act, which focuses on protecting fundamental rights, ensuring safety, and fostering innovation. Cross-referencing with the European Commission’s official AI Act overview and a European Parliament legislative summary (European Commission, 2024) confirms these goals. Both sources emphasize transparency, citizen protection, and the promotion of responsible AI development. Overall, the model’s explanation aligns well with official EU documentation and provides a reliable overview of the Act’s intent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc922",
   "metadata": {},
   "source": [
    "## Question 2: The Coffee Brewer\n",
    "\n",
    "You have bought a new coffee brewer, but, unfortunately, they only sent the operating instructions in Finnish. The instructions are available in `moccamaster_manual_fi.pdf` (available on Canvas).\n",
    "\n",
    "Add the document into `rag` and probe it to find out how to operate your coffee machine. Ask questions about things such as safety, operating, and cleaning of the machine, or what one should do if the machine breaks. In light of your understanding of coffee brewers and home appliances in general, do the answers make sense? Discuss and include at least one prompt and one answer from the LLM as part of your answer. Write 2–4 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda4f2a",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c70971",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_document(\"moccamaster_manual_fi.pdf\")\n",
    "response = rag.query(\"How to clean a coffee brewer?\")\n",
    "print(response[0])\n",
    "print(response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ae673",
   "metadata": {},
   "source": [
    "Prompt given to the model: How to clean a coffee brewer?\n",
    "\n",
    "Answer the model returned:According to the text, here are some instructions on how to clean a Moccamaster coffee brewer:\n",
    "\n",
    "1. Don't put the brewer in any kind of liquid.\n",
    "2. Clean the brewer regularly and evenly, especially from the inside.\n",
    "3. If the brewer is used daily, clean it every two weeks.\n",
    "4. Use only Clean Drop \"Coffee server cleaning tablet\" for cleaning, which can be purchased from Moccamaster dealers or online at www.moccamaster.com.\n",
    "5. Follow the instructions on the packaging of the cleaning tablet.\n",
    "\n",
    "Additionally, some specific cleaning tips are mentioned:\n",
    "\n",
    "* Don't clean the brewer with an astianpesukone (dishwasher).\n",
    "* Wash the inner parts of the brewer by hand with soap and water after each use.\n",
    "* se a dry cloth to wipe down the exterior of the brewer.\n",
    "* Avoid putting the brewer in hot water or any other liquid.\n",
    "\n",
    "It's also worth noting that Moccamaster coffee brewers have been certified by European Coffee Brewing Center (ECBC) for their high-quality and hygiene standards.\n",
    "\n",
    "The model reported using these internal refs: ['moccamaster_manual_fi.pdf:2:5', 'moccamaster_manual_fi.pdf:10:3', 'moccamaster_manual_fi.pdf:1:5', 'moccamaster_manual_fi.pdf:9:0', 'moccamaster_manual_fi.pdf:2:4']\n",
    "\n",
    "The answer appears consistent with general knowledge about maintaining coffee brewers. The official Moccamaster website confirms these instructions. Therefore, the model’s response matches both the manual and the company’s guidance, making it accurate and practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e88ad6",
   "metadata": {},
   "source": [
    "## Question 3: UFO files\n",
    "\n",
    "The file `ufo_files.zip` (available on Canvas) contains declassified CIA files pertaining to CIA's investigation into unidentified flying objects (UFOs).\n",
    "\n",
    "Extract the package and add the documents into your `rag`. Then, query whatever you think would be interesting to ask about the documents, such as where and when sightings have occurred, have there been spikes, does the collection contain information about publicly famous cases (such as the Roswell Incident), about governmental programs relating to UFOs, patterns in sightings, international relations, and so on.\n",
    "\n",
    "Look at the sources the model provides you. Do the sources support the inferences made by the model? Discuss, and again include at least one prompt and response pair. Write 2–4 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9130ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Path to the zip file\n",
    "zip_path = \"ufo_files.zip\"\n",
    "\n",
    "# Open the ZIP file and extract all contents\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"ufo_files\")\n",
    "\n",
    "rag.add_document(\"ufo_files\")\n",
    "response = rag.query(\"Does the collection contain information about publicly famous cases (such as the Roswell Incident), about governmental programs relating to UFOs, patterns in sightings, international relations?\")\n",
    "print(response[0])\n",
    "print(response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74338a89",
   "metadata": {},
   "source": [
    "Prompt given to the model: Does the collection contain information about publicly famous cases (such as the Roswell Incident), about governmental programs relating to UFOs, patterns in sightings, international relations?\"\n",
    "\n",
    "Answer the model returned:Based on the provided context, the answer is:\n",
    "\n",
    "Yes, the collection appears to contain information about both publicly famous cases and governmental programs relating to UFOs.\n",
    "\n",
    "Specifically, it mentions the Roswell Incident, a well-known case that has been extensively investigated and debated. Additionally, it references various government agencies involved in studying UFOs, such as the CIA, OSI, NSA, and DIA, which suggests that the collection may contain information about their activities and findings related to UFOs.\n",
    "\n",
    "Furthermore, the context mentions specific books and publications, such as \"UFO Files: The Untold Story\" and \"The Roswell Incident\", which are likely related to the publicly famous cases. It also references academic studies and investigations, such as the O'Brien Committee and the GAO investigation, which may provide insight into patterns in sightings and international relations.\n",
    "\n",
    "Overall, it appears that the collection is a compilation of information about various aspects of UFOs, including famous cases, government programs, and related research and investigations.\n",
    "\n",
    "The model reported using these internal refs: ['ufo_files/C05517742.pdf:17:3', 'ufo_files/C05517742.pdf:16:5', 'ufo_files/C05517742.pdf:17:6', 'ufo_files/C05517742.pdf:17:2', 'ufo_files/C05517742.pdf:17:4']\n",
    "\n",
    "The model found that the CIA UFO files reference well-known cases like Roswell and Project Blue Book, as well as agency involvement from the CIA, OSI, and Air Force. It also mentioned reports analyzing UFO sighting patterns and media coverage.\n",
    "\n",
    "These claims match what appears in the cited documents, which mainly summarize historical investigations and public reactions rather than confirm UFO activity. This aligns with information from the CIA FOIA Reading Room, where the files are described as declassified intelligence reports and press materials. Overall, the model’s answer is consistent with what is known about the CIA’s UFO records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a2c4d",
   "metadata": {},
   "source": [
    "## Question 4: Your own data\n",
    "\n",
    "Provide some documents of your own to the system. The data can be arbitrary, but preferably something that is widely not considered publicly available or googleable. It can be also something completely made up for the purpose, like a short story. **Suggestion:** If you come from another discipline than computer science or data science, try using some data within your own discipline. Perhaps a suitable research article? Anything will do.\n",
    "\n",
    "Present meaningful questions that can be answered with the data, and discuss the results. Write 2–4 paragraphs and again include at least one prompt and response pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939e913",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_document(\"demo.txt\")\n",
    "response = rag.query(\"Give. me the summary\")\n",
    "print(response[0])\n",
    "print(response[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
